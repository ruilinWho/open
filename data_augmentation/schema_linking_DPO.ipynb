{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearize(schema_links: list[dict[str, any]], column_choice: str) -> str:\n",
    "    result = {}\n",
    "\n",
    "    if column_choice == \"loss_columns\":\n",
    "        if all([len(t[\"loss_columns\"]) == 0 for t in schema_links]):\n",
    "            return None\n",
    "\n",
    "    for link in schema_links:\n",
    "        table = link[\"table_name\"]\n",
    "        columns = link[column_choice]\n",
    "        if len(columns) == 0:\n",
    "            continue\n",
    "        result[table] = columns\n",
    "\n",
    "    return \"```json\\n\" + json.dumps(result) + \"\\n```\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benches = [\"spider\", \"bird\"]\n",
    "\n",
    "\n",
    "def synthesize_dpo_data(ir: dict[str, any], datapoint: dict[str, any]) -> dict[str, any]:\n",
    "    \"\"\"\n",
    "    To build the DPO data, we use each schema-link gronud truth.\n",
    "    For each datapoint, we need to generate preference data: {base, result_loss}\n",
    "        - base: link[\"table_name\"] and link[\"related_columns\"]\n",
    "        - result_loss:\n",
    "            - base - random columns(tables are perfect)\n",
    "            - base - random tables\n",
    "    \"\"\"\n",
    "\n",
    "    schema_link: list = datapoint[\"schema_link\"]\n",
    "    tuned_schema_link = []\n",
    "\n",
    "    deletable_columns = []\n",
    "    # record all foreign key relationship, if we delete a table, we need to delete all the foreign key pointing to it\n",
    "    foreign_key_records: dict[str, list[tuple[str, str]]] = {}\n",
    "\n",
    "    for table in schema_link:\n",
    "        table_name = table[\"table_name\"]\n",
    "        related_columns = table[\"related_columns\"]\n",
    "\n",
    "        # Get all columns of this table (related and unrelated)\n",
    "        # NOTE: what we \"add\" to synthetic data should not be Primary key or Foreign key, so check it\n",
    "        table_ir = [t for t in ir[\"tables\"] if t[\"table_name\"] == table_name][0]\n",
    "        columns: list[dict] = [col for col in table_ir[\"columns\"]]\n",
    "        pk_indexes: list[int] = table_ir[\"primary_keys\"]\n",
    "        fks: list[dict] = table_ir[\"foreign_keys\"]\n",
    "\n",
    "        for fk in fks:\n",
    "            from_table = table_name\n",
    "            from_col = fk[\"column\"].strip('\"')\n",
    "            to_table = fk[\"referenced_table\"].strip('\"')\n",
    "            foreign_key_records.setdefault(to_table, []).append((from_table, from_col))\n",
    "\n",
    "        # Remove columns that are primary keys or foreign keys\n",
    "        for column in columns:\n",
    "            col_idx = column[\"col_idx\"]\n",
    "            col_name = column[\"col_name\"]\n",
    "\n",
    "            if (col_idx not in pk_indexes) and any([col_name == related_name for related_name in related_columns]):\n",
    "                deletable_columns.append({\"table_name\": table_name, \"column_name\": column[\"col_name\"]})\n",
    "\n",
    "        # Now we have the added columns, build the win data\n",
    "        tuned_schema_link.append(\n",
    "            {\n",
    "                \"table_name\": table_name,\n",
    "                \"related_columns\": related_columns,\n",
    "                \"win_columns\": copy.deepcopy(related_columns),\n",
    "                \"loss_columns\": copy.deepcopy(related_columns),  # copy here, delete later\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Now for every datapoint, we remove 1~2 columns to build the loss data\n",
    "    # 50% remove one column; 50% remove two columns\n",
    "\n",
    "    messed_results = []\n",
    "    tuned_schema_link_backup = copy.deepcopy(tuned_schema_link)\n",
    "\n",
    "    # Add K columns bad case\n",
    "    for _ in range(3):\n",
    "        tuned_schema_link = copy.deepcopy(tuned_schema_link_backup)\n",
    "\n",
    "        if len(deletable_columns) < 1:\n",
    "            break\n",
    "        elif len(deletable_columns) == 1:\n",
    "            to_deletes = [random.choice(deletable_columns)]\n",
    "        else:\n",
    "            if random.random() < 0.7:\n",
    "                to_deletes = [random.choice(deletable_columns)]\n",
    "            else:\n",
    "                to_deletes = random.sample(deletable_columns, 2)\n",
    "\n",
    "        for deleting in to_deletes:\n",
    "            for table in tuned_schema_link:\n",
    "                if table[\"table_name\"] == deleting[\"table_name\"]:\n",
    "                    table[\"loss_columns\"].remove(deleting[\"column_name\"])\n",
    "                    break\n",
    "\n",
    "        messed_results.append(\n",
    "            {\n",
    "                \"mess_type\": \"column\",\n",
    "                \"db_id\": datapoint[\"db_id\"],\n",
    "                \"question\": datapoint[\"question\"],\n",
    "                \"evidence\": datapoint[\"evidence\"],\n",
    "                \"schema\": datapoint[\"schema\"],\n",
    "                \"schema_link\": tuned_schema_link,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Remove current to_deletes\n",
    "        for deleting in to_deletes:\n",
    "            deletable_columns.remove(deleting)\n",
    "\n",
    "    if len(tuned_schema_link) == 1:\n",
    "        return messed_results\n",
    "\n",
    "    # (Table Lost DPO data)\n",
    "    for to_delete_table in tuned_schema_link_backup:\n",
    "        tuned_schema_link = copy.deepcopy(tuned_schema_link_backup)\n",
    "\n",
    "        for table in tuned_schema_link:\n",
    "            if table[\"table_name\"] == to_delete_table[\"table_name\"]:\n",
    "                # Set loss_columns to []\n",
    "                table[\"loss_columns\"] = []\n",
    "                break\n",
    "\n",
    "        to_delete_table_name = to_delete_table[\"table_name\"]\n",
    "        if to_delete_table_name in foreign_key_records:\n",
    "            to_delete_fks = foreign_key_records[to_delete_table_name]\n",
    "            for fk in to_delete_fks:\n",
    "                for table in tuned_schema_link:\n",
    "                    if table[\"table_name\"] == fk[0] and fk[1] in table[\"loss_columns\"]:\n",
    "                        table[\"loss_columns\"].remove(fk[1])\n",
    "\n",
    "        # Now we have the final data\n",
    "        messed_results.append(\n",
    "            {\n",
    "                \"mess_type\": \"table\",\n",
    "                \"db_id\": datapoint[\"db_id\"],\n",
    "                \"question\": datapoint[\"question\"],\n",
    "                \"evidence\": datapoint[\"evidence\"],\n",
    "                \"schema\": datapoint[\"schema\"],\n",
    "                \"schema_link\": tuned_schema_link,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return messed_results\n",
    "\n",
    "\n",
    "def build_dpo_data(bench: str):\n",
    "    assert bench in benches\n",
    "\n",
    "    if bench == \"bird\":\n",
    "        ir_path = \"<Anonymous>\"\n",
    "        schema_link_path = \"<Anonymous>\"\n",
    "        DATASET = json.load(open(\"<Anonymous>\"))\n",
    "    else:\n",
    "        ir_path = \"<Anonymous>\"\n",
    "        schema_link_path = \"<Anonymous>\"\n",
    "        DATASET = json.load(open(\"<Anonymous>\"))\n",
    "\n",
    "    ir_set = json.load(open(ir_path))\n",
    "    schema_links = json.load(open(schema_link_path))\n",
    "    print(\"IR length: \", len(ir_set))\n",
    "    print(\"Schema length: \", len(schema_links))\n",
    "\n",
    "    ########## Choose db_ids to build the DPO data ##########\n",
    "    all_db_ids = set([sl[\"db_id\"] for sl in schema_links])\n",
    "\n",
    "    # while True:\n",
    "    #     tmp_db_ids = [db_id for db_id in all_db_ids if (db_id != \"formula_1\" and db_id != \"scholar\")]\n",
    "\n",
    "    #     chosen_db_ids = random.sample(tmp_db_ids, int(len(all_db_ids) * 0.3))\n",
    "\n",
    "    #     chosen_question_count = 0\n",
    "    #     for db_id in chosen_db_ids:\n",
    "    #         chosen_question_count += len([q for q in DATASET if q[\"db_id\"] == db_id])\n",
    "    #     question_percent = chosen_question_count / len(schema_links)\n",
    "    #     if 0.24 < question_percent < 0.26:\n",
    "    #         print(\"Total question count: \", chosen_question_count)\n",
    "    #         print(\"DPO percent: \", question_percent, \"| SFT percent: \", 1 - question_percent, \"\\n\")\n",
    "    #         break\n",
    "    #     else:\n",
    "    #         pass\n",
    "    #         # print(\"Question percent: \", question_percent)\n",
    "    chosen_db_ids = []\n",
    "    ########## Build DPO data on specific db_id ##########\n",
    "\n",
    "    dpo_data = []\n",
    "    for db_id in chosen_db_ids:\n",
    "        try:\n",
    "            ir = [ir for ir in ir_set if ir[\"db_id\"] == db_id][0]\n",
    "        except:\n",
    "            print(\"Problem Occured: \", db_id)\n",
    "        datapoints = [sl for sl in schema_links if sl[\"db_id\"] == db_id]\n",
    "\n",
    "        for datapoint in datapoints:\n",
    "            # XXX: Considering this is a probabilistic algorithm, we do it multiple times\n",
    "            data = synthesize_dpo_data(ir, datapoint)\n",
    "            assert type(data) is list or data is None\n",
    "            if data is not None:\n",
    "                for datum in data:\n",
    "                    datum[\"bench\"] = bench\n",
    "                    datum[\"train_type\"] = \"DPO\"\n",
    "                    dpo_data.append(datum)\n",
    "\n",
    "    print(\"Bench = \", bench, \"| Total dpo_data length: \", len(dpo_data))\n",
    "\n",
    "    ########## Format the data ##########\n",
    "\n",
    "    train_data = []\n",
    "    for datum in dpo_data:\n",
    "        win_sl = linearize(datum[\"schema_link\"], \"win_columns\")\n",
    "        loss_sl = linearize(datum[\"schema_link\"], \"loss_columns\")\n",
    "\n",
    "        # If every table is empty, we skip this datapoint\n",
    "        if loss_sl is None:\n",
    "            continue\n",
    "\n",
    "        train_data.append(\n",
    "            {\n",
    "                \"bench\": bench,\n",
    "                \"train_type\": \"DPO\",\n",
    "                \"mess_type\": datum[\"mess_type\"],\n",
    "                \"db_id\": datum[\"db_id\"],\n",
    "                \"schema\": datum[\"schema\"],\n",
    "                \"question\": datum[\"question\"],\n",
    "                \"evidence\": datum[\"evidence\"],\n",
    "                \"win_sl\": win_sl,\n",
    "                \"loss_sl\": loss_sl,\n",
    "            }\n",
    "        )\n",
    "    # del datum #FIXME\n",
    "    ########## SFT data ##########\n",
    "    sft_data = []\n",
    "    sft_db_ids = all_db_ids - set(chosen_db_ids)\n",
    "\n",
    "    for sl in schema_links:\n",
    "        if sl[\"db_id\"] not in sft_db_ids:\n",
    "            continue\n",
    "\n",
    "        sft_data.append(\n",
    "            {\n",
    "                \"bench\": bench,\n",
    "                \"train_type\": \"SFT\",\n",
    "                \"db_id\": sl[\"db_id\"],\n",
    "                \"schema\": sl[\"schema\"],\n",
    "                \"question\": sl[\"question\"],\n",
    "                \"evidence\": sl[\"evidence\"],\n",
    "                \"standard_sl\": linearize(sl[\"schema_link\"], \"related_columns\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    mixed_data = train_data + sft_data\n",
    "\n",
    "    # OUTPUT_DIR = \"./dpo_data_fixfix\"\n",
    "    OUTPUT_DIR = \"./only_SFT\"\n",
    "    if not Path(OUTPUT_DIR).exists():\n",
    "        Path(OUTPUT_DIR).mkdir()\n",
    "\n",
    "    with open(f\"{OUTPUT_DIR}/{bench}_mixed.json\", \"w\") as f:\n",
    "        json.dump(mixed_data, f, indent=2)\n",
    "\n",
    "    return mixed_data\n",
    "\n",
    "\n",
    "spider_mix = build_dpo_data(\"spider\")\n",
    "bird_mix = build_dpo_data(\"bird\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mix = spider_mix + bird_mix\n",
    "with open(\"./only_SFT/all_mixed_SFT.json\", \"w\") as f:\n",
    "    json.dump(all_mix, f, indent=2)\n",
    "    print(\"Mixed data length: \", len(all_mix))\n",
    "    print(\"DPO data number = \", len([d for d in all_mix if d[\"train_type\"] == \"DPO\"]))\n",
    "    print(\"SFT data number = \", len([d for d in all_mix if d[\"train_type\"] == \"SFT\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
